{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn.classifier\n",
    "import nn.unet_origin as unet_origin\n",
    "import nn.unet as unet\n",
    "import torch.optim as optim\n",
    "import helpers\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "import img.augmentation as aug\n",
    "from data.fetcher import DatasetFetcher\n",
    "import nn.classifier\n",
    "from nn.train_callbacks import TensorboardVisualizerCallback, TensorboardLoggerCallback, ModelSaverCallback\n",
    "from nn.test_callbacks import PredictionsSaverCallback\n",
    "\n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from data.dataset import TrainImageDataset, TestImageDataset\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_img_resize = (64, 64) # The resize size of the input images of the neural net\n",
    "output_img_resize = (64, 64) # The resize size of the output images of the neural net\n",
    "batch_size = 2\n",
    "epochs = 150\n",
    "threshold = 0.5\n",
    "validation_size = 0.1\n",
    "sample_size = None\n",
    "\n",
    "# -- Optional parameters\n",
    "threads = cpu_count()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "script_dir = 'C:\\\\User\\\\ZhangX1\\\\Documents\\\\Python_Scripts\\\\DeepLearn\\\\src'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets are present.\n"
     ]
    }
   ],
   "source": [
    "# Fetch the datasets\n",
    "ds_fetcher = DatasetFetcher()\n",
    "ds_fetcher.fetch_dataset()\n",
    "\n",
    "# Get the path to the files for the neural net\n",
    "# We don't want to split train/valid for KFold crossval\n",
    "X_train, y_train, X_valid, y_valid = ds_fetcher.get_train_files(sample_size=sample_size,\n",
    "                                                                validation_size=validation_size)\n",
    "full_x_test = ds_fetcher.get_test_files(sample_size)\n",
    "\n",
    "# -- Computed parameters\n",
    "# Get the original images size (assuming they are all the same size)\n",
    "origin_img_size = ds_fetcher.get_image_size(X_train[0])\n",
    "    \n",
    "train_ds = TrainImageDataset(X_train, y_train, input_img_resize, X_transform=None)\n",
    "train_loader = DataLoader(train_ds, batch_size,\n",
    "                          sampler=RandomSampler(train_ds),\n",
    "                          num_workers=threads,\n",
    "                          pin_memory=use_cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "import img.transformer as transformer\n",
    "\n",
    "img = Image.open(train_loader.dataset.X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = (64, 64)\n",
    "img = transformer.center_cropping_resize(img, img_resize)\n",
    "img = np.asarray(img.convert(\"L\"), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.,  26.,  23., ...,  10.,  93., 188.],\n",
       "       [ 34.,  28.,  22., ...,  67., 102., 100.],\n",
       "       [ 33.,  26.,  22., ...,  90., 116., 105.],\n",
       "       ...,\n",
       "       [ 34.,  52.,  75., ...,  90.,  97., 118.],\n",
       "       [ 35.,  47.,  64., ...,  92., 106.,  66.],\n",
       "       [ 38.,  48.,  67., ...,  95., 122.,  13.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img.shape\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Image.open(train_loader.dataset.y_train_masks[0])\n",
    "mask = transformer.center_cropping_resize(mask, img_resize)\n",
    "mask = np.asarray(mask.convert(\"L\"), dtype=np.float32)  # GreyScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert training image and mask to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "tensor_img = transformer.image_to_tensor(img)\n",
    "#tensor_mask = transformer.mask_to_tensor(mask, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before that, check the dimension of RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "example_file = glob('../../carvana-challenge/input/*.jpg')\n",
    "example_img = Image.open(example_file[0])    \n",
    "example_img = np.asarray(example_img.convert(\"RGB\"), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1280, 1918)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_img.transpose((2, 0, 1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-step tensor conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std = 1\n",
    "image = img.astype(np.float32)\n",
    "image = (image - mean) / std #There is no scaling in this case\n",
    "image = np.expand_dims(image, axis=0) #(1, 64, 64)\n",
    "tensor = torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = mask\n",
    "mask1 = (mask1 > threshold).astype(np.float32)\n",
    "tensor = torch.from_numpy(mask1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing get_mask_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a mask and an image this method returns\n",
    "one image representing 3 patches of the same image.\n",
    "\"\"\"\n",
    "\n",
    "image = img.astype(np.float32)\n",
    "image = (image - mean) / std #There is no scaling in this case\n",
    "image = np.expand_dims(image, axis=0) #(1, 64, 64)\n",
    "image = image.transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc as scipy\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "H, W, C = image.shape\n",
    "results = np.zeros((H, 3 * W, 1), np.uint8)  # shape is (64, 192, 1)\n",
    "p = np.zeros((H * W, 1), np.uint8)  # shape is (4096, 1)\n",
    "\n",
    "m = np.zeros((H * W), np.uint8)\n",
    "l = mask1.reshape(-1)\n",
    "\n",
    "color = 255\n",
    "mask2 = np.expand_dims(mask1, axis=2) * np.array(color)\n",
    "mask2 = mask2.astype(np.uint8)\n",
    "# masked_img = self._apply_mask_overlay(image, mask)\n",
    "\n",
    "a = (2 * l + m)\n",
    "miss = np.where(a == 2)[0]\n",
    "hit = np.where(a == 3)[0]\n",
    "fp = np.where(a == 1)[0]\n",
    "\n",
    "masked_img = cv2.addWeighted(mask, 0.5, image, 0.5, 0.)\n",
    "p[miss] = np.array([255])\n",
    "p[hit] = np.array([128])\n",
    "p[fp] = np.array([0])\n",
    "p = p.reshape(H, W, 1)\n",
    "masked_img = np.expand_dims(masked_img, axis = 2)\n",
    "results[:, 0:W] = image\n",
    "results[:, W:2 * W] = p\n",
    "results[:, 2 * W:3 * W] = masked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 192, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
